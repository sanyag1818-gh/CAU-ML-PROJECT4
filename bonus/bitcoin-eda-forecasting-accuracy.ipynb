{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n\n# Part 1: Data Loading and Basic Processing\nprint(\"Step 1: Loading Data\")\n# Load data - use the file you uploaded to Jupyter\nbtc = pd.read_csv(\"BTC-USD.csv\", parse_dates=[\"Date\"])\nbtc = btc.sort_values(\"Date\").reset_index(drop=True)\nbtc[\"Date\"] = pd.to_datetime(btc[\"Date\"])\n\nprint(f\"Original data shape: {btc.shape}\")\nprint(f\"Date range: {btc['Date'].min()} to {btc['Date'].max()}\")\nprint(\"\\nFirst 5 rows:\")\nprint(btc.head())\n\n# Part 2: Feature Engineering\nprint(\"Step 2: Feature Engineering\")\n\n# 1. Calculate returns\nbtc[\"returns\"] = btc[\"Close\"].pct_change()\n\n# 2. Calculate excess returns (simplified: risk-free rate = 0)\nrisk_free_rate = 0.0\nbtc[\"excess_returns\"] = btc[\"returns\"] - risk_free_rate\n\n# 3. Target variable: 5-day future excess returns\nhorizon = 5\nbtc[\"target_excess_ret\"] = btc[\"excess_returns\"].shift(-horizon)\n\n# 4. Technical indicator features\nbtc[\"ret_1d\"] = btc[\"returns\"]\nbtc[\"ret_5d\"] = btc[\"Close\"].pct_change(5)\nbtc[\"ret_20d\"] = btc[\"Close\"].pct_change(20)\n\n# Price range features\nbtc[\"range_pct\"] = (btc[\"High\"] - btc[\"Low\"]) / btc[\"Close\"]\nbtc[\"upper_shadow\"] = (btc[\"High\"] - btc[\"Close\"]) / btc[\"Close\"]\nbtc[\"lower_shadow\"] = (btc[\"Close\"] - btc[\"Low\"]) / btc[\"Close\"]\n\n# Volatility features\nbtc[\"vol_5d\"] = btc[\"returns\"].rolling(5).std()\nbtc[\"vol_20d\"] = btc[\"returns\"].rolling(20).std()\n\n# Volume features\nbtc[\"vol_chg\"] = btc[\"Volume\"].pct_change()\nbtc[\"vol_ma5\"] = btc[\"Volume\"].rolling(5).mean() / btc[\"Volume\"]\n\n# Moving average features\nbtc[\"ma_5\"] = btc[\"Close\"].rolling(5).mean() / btc[\"Close\"]\nbtc[\"ma_20\"] = btc[\"Close\"].rolling(20).mean() / btc[\"Close\"]\n\n# Remove missing values\nbtc = btc.dropna()\n\n# Take first 500 samples (avoid excessive data size)\nbtc = btc.iloc[:500].copy()\n\nprint(f\"Processed data shape: {btc.shape}\")\nprint(f\"Feature list: {btc.columns.tolist()}\")\n\n# Part 3: Prepare Training Data\nprint(\"Step 3: Preparing Training Data\")\n\n# Select features\nfeatures = [\n    \"ret_1d\", \"ret_5d\", \"ret_20d\",\n    \"range_pct\", \"upper_shadow\", \"lower_shadow\",\n    \"vol_5d\", \"vol_20d\", \"vol_chg\", \"vol_ma5\",\n    \"ma_5\", \"ma_20\"\n]\n\nX = btc[features].values\ny = btc[\"target_excess_ret\"].values\ndates = btc[\"Date\"].values\nactual_returns = btc[\"excess_returns\"].values\n\nprint(f\"Feature matrix X shape: {X.shape}\")\nprint(f\"Target variable y shape: {y.shape}\")\n\n# Data normalization\nscaler_X = MinMaxScaler()\nscaler_y = MinMaxScaler()\n\nX_scaled = scaler_X.fit_transform(X)\ny_scaled = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n\n# Split train/test sets (80/20)\nsplit_idx = int(len(X_scaled) * 0.8)\n\nX_train = X_scaled[:split_idx]\nX_test = X_scaled[split_idx:]\ny_train = y_scaled[:split_idx]\ny_test = y_scaled[split_idx:]\n\n# Save test set actual returns (for later strategy evaluation)\ntest_returns = actual_returns[split_idx:]\ntest_dates = dates[split_idx:]\n\nprint(f\"Training set size: {len(X_train)}\")\nprint(f\"Test set size: {len(X_test)}\")\n\n# Reshape for LSTM input format (samples, timesteps, features)\nlookback = 1  # Use 1 timestep\nX_train = X_train.reshape((X_train.shape[0], lookback, X_train.shape[1]))\nX_test = X_test.reshape((X_test.shape[0], lookback, X_test.shape[1]))\n\nprint(f\"LSTM input shape: {X_train.shape}\")\n\n# ========================================\n# Part 4: Build and Train Model\n# ========================================\nprint(\"Step 4: Building and Training Model\")\n# Build improved LSTM model\nmodel = keras.Sequential([\n    keras.layers.LSTM(128, return_sequences=True, \n                      input_shape=(lookback, len(features))),\n    keras.layers.Dropout(0.2),\n    keras.layers.LSTM(64),\n    keras.layers.Dropout(0.2),\n    keras.layers.Dense(32, activation='relu'),\n    keras.layers.Dense(1)\n])\n\nmodel.compile(optimizer='adam', loss='mse', metrics=['mae'])\nmodel.summary()\n\n# Train model\nprint(\"\\nStarting training...\")\nhistory = model.fit(\n    X_train, y_train,\n    epochs=30,\n    batch_size=32,\n    validation_split=0.1,\n    verbose=1\n)\n\n# ========================================\n# Part 5: Prediction\n# ========================================\nprint(\"Step 5: Generating Predictions\")\ny_pred_scaled = model.predict(X_test, verbose=0)\ny_pred = scaler_y.inverse_transform(y_pred_scaled).flatten()\ny_test_original = scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()\n\nprint(f\"Prediction range: [{y_pred.min():.4f}, {y_pred.max():.4f}]\")\nprint(f\"Actual value range: [{y_test_original.min():.4f}, {y_test_original.max():.4f}]\")\n\n# ========================================\n# Part 6: Portfolio Strategy\n# ========================================\nprint(\"Step 6: Implementing Portfolio Strategy\")\n\ndef compute_weights(predicted_returns, risk_aversion=10):\n    \"\"\"\n    Calculate allocation weights (0-2) based on predicted returns\n    \"\"\"\n    weights = []\n    for pred in predicted_returns:\n        if pred > 0:\n            # Positive return: weight between 0-2, higher return = higher weight\n            weight = min(2.0, max(0, pred * risk_aversion))\n        else:\n            # Negative return: weight = 0\n            weight = 0.0\n        weights.append(weight)\n    return np.array(weights)\n\n# Calculate initial weights\ninitial_weights = compute_weights(y_pred, risk_aversion=10)\n\n# Calculate strategy returns\n# Note: Using actual returns after prediction time for strategy performance\nstrategy_returns = initial_weights * test_returns\n\n# Benchmark returns (buy & hold strategy)\nbenchmark_returns = test_returns\n\n# Calculate volatility\nstrategy_vol = np.std(strategy_returns)\nbenchmark_vol = np.std(benchmark_returns)\nvol_ratio = strategy_vol / benchmark_vol\n\nprint(f\"Strategy volatility: {strategy_vol:.6f}\")\nprint(f\"Benchmark volatility: {benchmark_vol:.6f}\")\nprint(f\"Volatility ratio: {vol_ratio:.3f}\")\n\n# Apply volatility constraint (≤ 1.2× benchmark volatility)\nif vol_ratio > 1.2:\n    print(\"\\n  Violates volatility constraint, adjusting weights...\")\n    adjustment_factor = 1.2 / vol_ratio\n    adjusted_weights = initial_weights * adjustment_factor\n    strategy_returns = adjusted_weights * test_returns\n    \n    # Recalculate volatility\n    strategy_vol = np.std(strategy_returns)\n    vol_ratio = strategy_vol / benchmark_vol\n    print(f\"Adjusted volatility ratio: {vol_ratio:.3f}\")\nelse:\n    print(\"✓ Satisfies volatility constraint\")\n    adjusted_weights = initial_weights\n\n# ========================================\n# Part 7: Evaluation Metrics\n# ========================================\nprint(\"Step 7: Computing Evaluation Metrics\")\n\n# 1. Sharpe Ratio (annualized)\ndef sharpe_ratio(returns, risk_free=0):\n    excess = returns - risk_free\n    if np.std(excess) == 0:\n        return 0\n    return np.mean(excess) / np.std(excess) * np.sqrt(252)\n\nstrategy_sharpe = sharpe_ratio(strategy_returns)\nbenchmark_sharpe = sharpe_ratio(benchmark_returns)\n\n# 2. Cumulative returns\nstrategy_cumret = (1 + strategy_returns).prod() - 1\nbenchmark_cumret = (1 + benchmark_returns).prod() - 1\n\n# 3. Maximum drawdown\ndef max_drawdown(returns):\n    cum_returns = (1 + returns).cumprod()\n    running_max = np.maximum.accumulate(cum_returns)\n    drawdown = (cum_returns - running_max) / running_max\n    return drawdown.min()\n\nstrategy_mdd = max_drawdown(strategy_returns)\nbenchmark_mdd = max_drawdown(benchmark_returns)\n\n# 4. Annualized return\ndef annualized_return(returns):\n    total_return = (1 + returns).prod()\n    n_days = len(returns)\n    return total_return ** (252 / n_days) - 1\n\nstrategy_ann_ret = annualized_return(strategy_returns)\nbenchmark_ann_ret = annualized_return(benchmark_returns)\n\n# Print results\nprint(\"Evaluation Results Summary\")\nprint(f\"\\n{'Metric':<20} {'Strategy':<15} {'Benchmark':<15}\")\nprint(f\"{'Sharpe Ratio':<20} {strategy_sharpe:>14.3f} {benchmark_sharpe:>14.3f}\")\nprint(f\"{'Cumulative Return':<20} {strategy_cumret:>13.2%} {benchmark_cumret:>13.2%}\")\nprint(f\"{'Annualized Return':<20} {strategy_ann_ret:>13.2%} {benchmark_ann_ret:>13.2%}\")\nprint(f\"{'Maximum Drawdown':<20} {strategy_mdd:>13.2%} {benchmark_mdd:>13.2%}\")\nprint(f\"{'Volatility':<20} {strategy_vol:>14.6f} {benchmark_vol:>14.6f}\")\nprint(f\"{'Volatility Ratio':<20} {vol_ratio:>14.3f} {'1.000':>14}\")\n\n# ========================================\n# Part 8: Visualization\n# ========================================\n\nprint(\"Step 8: Generating Visualizations\")\n\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n# 1. Training loss curve\naxes[0, 0].plot(history.history['loss'], label='Training Loss')\naxes[0, 0].plot(history.history['val_loss'], label='Validation Loss')\naxes[0, 0].set_title('Model Training Loss')\naxes[0, 0].set_xlabel('Epoch')\naxes[0, 0].set_ylabel('Loss')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\n# 2. Cumulative returns comparison\nstrategy_cum = (1 + strategy_returns).cumprod()\nbenchmark_cum = (1 + benchmark_returns).cumprod()\n\naxes[0, 1].plot(strategy_cum, label=f'Strategy (Sharpe: {strategy_sharpe:.2f})', linewidth=2)\naxes[0, 1].plot(benchmark_cum, label=f'Buy & Hold (Sharpe: {benchmark_sharpe:.2f})', linewidth=2)\naxes[0, 1].set_title('Cumulative Returns Comparison')\naxes[0, 1].set_xlabel('Days')\naxes[0, 1].set_ylabel('Cumulative Return')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# 3. Weight distribution\naxes[1, 0].hist(adjusted_weights, bins=30, edgecolor='black', alpha=0.7)\naxes[1, 0].axvline(adjusted_weights.mean(), color='red', \n                   linestyle='--', label=f'Mean: {adjusted_weights.mean():.2f}')\naxes[1, 0].set_title('Portfolio Weights Distribution')\naxes[1, 0].set_xlabel('Weight')\naxes[1, 0].set_ylabel('Frequency')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\n# 4. Drawdown curve\ndef compute_drawdown(returns):\n    cum_returns = (1 + returns).cumprod()\n    running_max = np.maximum.accumulate(cum_returns)\n    drawdown = (cum_returns - running_max) / running_max\n    return drawdown\n\nstrategy_dd = compute_drawdown(strategy_returns)\nbenchmark_dd = compute_drawdown(benchmark_returns)\n\naxes[1, 1].plot(strategy_dd, label=f'Strategy (Max: {strategy_mdd:.2%})', linewidth=2)\naxes[1, 1].plot(benchmark_dd, label=f'Benchmark (Max: {benchmark_mdd:.2%})', linewidth=2)\naxes[1, 1].set_title('Drawdown Comparison')\naxes[1, 1].set_xlabel('Days')\naxes[1, 1].set_ylabel('Drawdown')\naxes[1, 1].legend()\naxes[1, 1].grid(True, alpha=0.3)\naxes[1, 1].axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Summary\")\nprint(f\"✓ Data processing: {len(btc)} samples\")\nprint(f\"✓ Feature engineering: {len(features)} f","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}